############################################################
Evaluating: CLIP-ViT-B/32
############################################################
Loading CLIP base model: ViT-B/32

============================================================
Evaluating on Caltech-101
============================================================
Evaluating: 100%|███████████████████████████████| 34/34 [00:07<00:00,  4.46it/s]
Caltech-101: 85.63%

============================================================
Evaluating on CIFAR-10
============================================================
Evaluating: 100%|███████████████████████████████| 40/40 [00:07<00:00,  5.58it/s]
CIFAR-10: 88.32%

============================================================
Evaluating on Flowers-102
============================================================
Error evaluating Flowers-102: <urlopen error [Errno -3] Temporary failure in name resolution>

============================================================
Evaluating on CIFAR-100
============================================================
Evaluating: 100%|███████████████████████████████| 40/40 [00:07<00:00,  5.68it/s]
CIFAR-100: 64.43%

============================================================
Evaluating on Oxford Pets
============================================================
Error evaluating Oxford Pets: <urlopen error [Errno -3] Temporary failure in name resolution>

############################################################
Evaluating: CoN-CLIP-ViT-B/32
############################################################
Loading CLIP base model: ViT-B/32
Loading checkpoint from /home/pankaja/ENTC/Sem5/CoN-CLIP-Project/checkpoints/conclip_b32/ckpt_5_conclip_b32.pt
Done Loading Checkpoint
Checkpoint type: <class 'dict'>
Checkpoint keys: dict_keys(['model', 'optimizer', 'args'])
Loading state dict from 'model' key in checkpoint

============================================================
Evaluating on Caltech-101
============================================================
Evaluating: 100%|███████████████████████████████| 34/34 [00:14<00:00,  2.36it/s]
Caltech-101: 87.81%

============================================================
Evaluating on CIFAR-10
============================================================
Evaluating: 100%|███████████████████████████████| 40/40 [00:15<00:00,  2.52it/s]
CIFAR-10: 89.93%

============================================================
Evaluating on Flowers-102
============================================================
Error evaluating Flowers-102: <urlopen error [Errno -3] Temporary failure in name resolution>

============================================================
Evaluating on CIFAR-100
============================================================
Evaluating: 100%|███████████████████████████████| 40/40 [00:16<00:00,  2.44it/s]
CIFAR-100: 64.75%

============================================================
Evaluating on Oxford Pets
============================================================
Error evaluating Oxford Pets: <urlopen error [Errno -3] Temporary failure in name resolution>

############################################################
Evaluating: CLIP-ViT-B/16
############################################################
Loading CLIP base model: ViT-B/16

============================================================
Evaluating on Caltech-101
============================================================
Evaluating: 100%|███████████████████████████████| 34/34 [00:23<00:00,  1.48it/s]
Caltech-101: 84.42%

============================================================
Evaluating on CIFAR-10
============================================================
Evaluating: 100%|███████████████████████████████| 40/40 [00:25<00:00,  1.57it/s]
CIFAR-10: 90.10%

============================================================
Evaluating on Flowers-102
============================================================
100%|████████████████████████████████████████| 345M/345M [01:34<00:00, 3.66MB/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 502/502 [00:00<00:00, 1.49MB/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15.0k/15.0k [00:00<00:00, 36.9MB/s]
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:15<00:00,  1.58it/s]
Flowers-102: 67.70%

============================================================
Evaluating on CIFAR-100
============================================================
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:24<00:00,  1.65it/s]
CIFAR-100: 68.41%

============================================================
Evaluating on Oxford Pets
============================================================
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792M/792M [03:10<00:00, 4.15MB/s]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19.2M/19.2M [00:11<00:00, 1.62MB/s]
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:10<00:00,  1.49it/s]
Oxford Pets: 88.23%

############################################################
Evaluating: CoN-CLIP-ViT-B/16
############################################################
Loading CLIP base model: ViT-B/16
Loading checkpoint from /home/pankaja/ENTC/Sem5/CoN-CLIP-Project/checkpoints/conclip_b16/ckpt_5_conclip_b16.pt
Done Loading Checkpoint
Checkpoint type: <class 'dict'>
Checkpoint keys: dict_keys(['model', 'optimizer', 'args'])
Loading state dict from 'model' key in checkpoint

============================================================
Evaluating on Caltech-101
============================================================
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:54<00:00,  1.59s/it]
Caltech-101: 89.13%

============================================================
Evaluating on CIFAR-10
============================================================
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [01:02<00:00,  1.56s/it]
CIFAR-10: 91.55%

============================================================
Evaluating on Flowers-102
============================================================
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:41<00:00,  1.65s/it]
Flowers-102: 68.09%

============================================================
Evaluating on CIFAR-100
============================================================
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [01:03<00:00,  1.59s/it]
CIFAR-100: 69.31%

============================================================
Evaluating on Oxford Pets
============================================================
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:27<00:00,  1.83s/it]
Oxford Pets: 88.72%

############################################################
Evaluating: CLIP-ViT-L/14
############################################################
Loading CLIP base model: ViT-L/14

============================================================
Evaluating on Caltech-101
============================================================
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [01:26<00:00,  2.54s/it]
Caltech-101: 85.54%

============================================================
Evaluating on CIFAR-10
============================================================
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [01:36<00:00,  2.41s/it]
CIFAR-10: 95.17%

============================================================
Evaluating on Flowers-102
============================================================
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [01:03<00:00,  2.54s/it]
Flowers-102: 74.52%

============================================================
Evaluating on CIFAR-100
============================================================
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [01:37<00:00,  2.43s/it]
CIFAR-100: 77.17%

============================================================
Evaluating on Oxford Pets
============================================================
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:40<00:00,  2.67s/it]
Oxford Pets: 93.08%

############################################################
Evaluating: CoN-CLIP-ViT-L/14
############################################################
Loading CLIP base model: ViT-L/14
Loading checkpoint from /home/pankaja/ENTC/Sem5/CoN-CLIP-Project/checkpoints/conclip_l14/ckpt_5_conclip_l14.pt
Done Loading Checkpoint
Traceback (most recent call last):
  File "/home/pankaja/ENTC/Sem5/CoN-CLIP-Project/src/benchmark_tests.py", line 575, in <module>
  File "/home/pankaja/ENTC/Sem5/CoN-CLIP-Project/src/benchmark_tests.py", line 501, in run_comparison
  File "/home/pankaja/ENTC/Sem5/CoN-CLIP-Project/src/benchmark_tests.py", line 413, in load_model
    else:
  File "/home/pankaja/cv/lib/python3.10/site-packages/torch/serialization.py", line 1530, in load
    return _load(
  File "/home/pankaja/cv/lib/python3.10/site-packages/torch/serialization.py", line 2119, in _load
    result = unpickler.load()
  File "/home/pankaja/cv/lib/python3.10/site-packages/torch/serialization.py", line 2083, in persistent_load
    typed_storage = load_tensor(
  File "/home/pankaja/cv/lib/python3.10/site-packages/torch/serialization.py", line 2049, in load_tensor
    wrap_storage = restore_location(storage, location)
  File "/home/pankaja/cv/lib/python3.10/site-packages/torch/serialization.py", line 1859, in restore_location
    return default_restore_location(storage, map_location)
  File "/home/pankaja/cv/lib/python3.10/site-packages/torch/serialization.py", line 698, in default_restore_location
    result = fn(storage, location)
  File "/home/pankaja/cv/lib/python3.10/site-packages/torch/serialization.py", line 637, in _deserialize
    return obj.to(device=device)
  File "/home/pankaja/cv/lib/python3.10/site-packages/torch/storage.py", line 291, in to
    return _to(self, device, non_blocking)
  File "/home/pankaja/cv/lib/python3.10/site-packages/torch/_utils.py", line 101, in _to
    untyped_storage = torch.UntypedStorage(self.size(), device=device)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacity of 7.62 GiB of which 59.62 MiB is free. Including non-PyTorch memory, this process has 7.54 GiB memory in use. Of the allocated memory 7.23 GiB is allocated by PyTorch, and 197.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
############################################################